Setting up a new session...
----------------- Options ---------------
               batch_size: 512                           	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/gaugan/            	[default: None]
             dataset_mode: aligned                       
                direction: BtoA                          	[default: AtoB]
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: vanilla                       
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 100.0                         
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: pix2pix                       	[default: cycle_gan]
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: gaugan_pix2pix                	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: False                         
                  no_html: False                         
                     norm: batch                         
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 0                             
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 148
initialize network with normal
initialize network with normal
model [Pix2PixModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.414 M
[Network D] Total number of parameters : 2.769 M
-----------------------------------------------
create web directory ./checkpoints/gaugan_pix2pix/web...
learning rate 0.0002000 -> 0.0002000
/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
End of epoch 1 / 200 	 Time Taken: 30 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 2 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 3 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 4 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 5, iters 2560
End of epoch 5 / 200 	 Time Taken: 3 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 6 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 7 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 8 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 9 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 10, iters 5120
End of epoch 10 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 11 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 12 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 13 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 14 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 15, iters 7680
End of epoch 15 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 16 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 17 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 18 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 19 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 20, iters 10240
End of epoch 20 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 21 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 22 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 23 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 24 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 25, iters: 512, time: 0.001, data: 1.521) G_GAN: 1.047 G_L1: 18.071 D_real: 0.615 D_fake: 0.521 
saving the model at the end of epoch 25, iters 12800
End of epoch 25 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 26 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 27 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 28 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 29 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 30, iters 15360
End of epoch 30 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 31 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 32 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 33 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 34 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 35, iters 17920
End of epoch 35 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 36 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 37 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 38 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 39 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 40, iters 20480
End of epoch 40 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 41 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 42 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 43 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 44 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 45, iters 23040
End of epoch 45 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 46 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 47 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 48 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 49 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 50, iters: 512, time: 0.001, data: 1.788) G_GAN: 1.055 G_L1: 15.193 D_real: 0.593 D_fake: 0.629 
saving the model at the end of epoch 50, iters 25600
End of epoch 50 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 51 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 52 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 53 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 54 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 55, iters 28160
End of epoch 55 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 56 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 57 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 58 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 59 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 60, iters 30720
End of epoch 60 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 61 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 62 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 63 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 64 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 65, iters 33280
End of epoch 65 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 66 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 67 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 68 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 69 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 70, iters 35840
End of epoch 70 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 71 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 72 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 73 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 74 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 75, iters: 512, time: 0.001, data: 1.848) G_GAN: 0.936 G_L1: 13.544 D_real: 0.610 D_fake: 0.584 
saving the model at the end of epoch 75, iters 38400
End of epoch 75 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 76 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 77 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 78 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 79 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 80, iters 40960
End of epoch 80 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 81 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 82 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 83 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 84 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 85, iters 43520
End of epoch 85 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 86 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 87 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 88 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 89 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 90, iters 46080
End of epoch 90 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 91 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 92 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 93 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 94 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 95, iters 48640
End of epoch 95 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 96 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 97 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 98 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 99 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0001980
(epoch: 100, iters: 512, time: 0.002, data: 1.822) G_GAN: 0.976 G_L1: 12.177 D_real: 0.486 D_fake: 0.565 
saving the model at the end of epoch 100, iters 51200
End of epoch 100 / 200 	 Time Taken: 4 sec
learning rate 0.0001980 -> 0.0001960
End of epoch 101 / 200 	 Time Taken: 2 sec
learning rate 0.0001960 -> 0.0001941
End of epoch 102 / 200 	 Time Taken: 2 sec
learning rate 0.0001941 -> 0.0001921
End of epoch 103 / 200 	 Time Taken: 2 sec
learning rate 0.0001921 -> 0.0001901
End of epoch 104 / 200 	 Time Taken: 2 sec
learning rate 0.0001901 -> 0.0001881
saving the model at the end of epoch 105, iters 53760
End of epoch 105 / 200 	 Time Taken: 4 sec
learning rate 0.0001881 -> 0.0001861
End of epoch 106 / 200 	 Time Taken: 2 sec
learning rate 0.0001861 -> 0.0001842
End of epoch 107 / 200 	 Time Taken: 2 sec
learning rate 0.0001842 -> 0.0001822
End of epoch 108 / 200 	 Time Taken: 2 sec
learning rate 0.0001822 -> 0.0001802
End of epoch 109 / 200 	 Time Taken: 2 sec
learning rate 0.0001802 -> 0.0001782
saving the model at the end of epoch 110, iters 56320
End of epoch 110 / 200 	 Time Taken: 4 sec
learning rate 0.0001782 -> 0.0001762
End of epoch 111 / 200 	 Time Taken: 2 sec
learning rate 0.0001762 -> 0.0001743
End of epoch 112 / 200 	 Time Taken: 2 sec
learning rate 0.0001743 -> 0.0001723
End of epoch 113 / 200 	 Time Taken: 2 sec
learning rate 0.0001723 -> 0.0001703
End of epoch 114 / 200 	 Time Taken: 2 sec
learning rate 0.0001703 -> 0.0001683
saving the model at the end of epoch 115, iters 58880
End of epoch 115 / 200 	 Time Taken: 4 sec
learning rate 0.0001683 -> 0.0001663
End of epoch 116 / 200 	 Time Taken: 2 sec
learning rate 0.0001663 -> 0.0001644
End of epoch 117 / 200 	 Time Taken: 2 sec
learning rate 0.0001644 -> 0.0001624
End of epoch 118 / 200 	 Time Taken: 2 sec
learning rate 0.0001624 -> 0.0001604
End of epoch 119 / 200 	 Time Taken: 2 sec
learning rate 0.0001604 -> 0.0001584
saving the model at the end of epoch 120, iters 61440
End of epoch 120 / 200 	 Time Taken: 4 sec
learning rate 0.0001584 -> 0.0001564
End of epoch 121 / 200 	 Time Taken: 2 sec
learning rate 0.0001564 -> 0.0001545
End of epoch 122 / 200 	 Time Taken: 2 sec
learning rate 0.0001545 -> 0.0001525
End of epoch 123 / 200 	 Time Taken: 2 sec
learning rate 0.0001525 -> 0.0001505
End of epoch 124 / 200 	 Time Taken: 2 sec
learning rate 0.0001505 -> 0.0001485
(epoch: 125, iters: 512, time: 0.002, data: 1.822) G_GAN: 1.032 G_L1: 11.458 D_real: 0.548 D_fake: 0.350 
saving the model at the end of epoch 125, iters 64000
End of epoch 125 / 200 	 Time Taken: 4 sec
learning rate 0.0001485 -> 0.0001465
End of epoch 126 / 200 	 Time Taken: 2 sec
learning rate 0.0001465 -> 0.0001446
End of epoch 127 / 200 	 Time Taken: 2 sec
learning rate 0.0001446 -> 0.0001426
End of epoch 128 / 200 	 Time Taken: 2 sec
learning rate 0.0001426 -> 0.0001406
End of epoch 129 / 200 	 Time Taken: 2 sec
learning rate 0.0001406 -> 0.0001386
saving the model at the end of epoch 130, iters 66560
End of epoch 130 / 200 	 Time Taken: 4 sec
learning rate 0.0001386 -> 0.0001366
End of epoch 131 / 200 	 Time Taken: 2 sec
learning rate 0.0001366 -> 0.0001347
End of epoch 132 / 200 	 Time Taken: 2 sec
learning rate 0.0001347 -> 0.0001327
End of epoch 133 / 200 	 Time Taken: 2 sec
learning rate 0.0001327 -> 0.0001307
End of epoch 134 / 200 	 Time Taken: 2 sec
learning rate 0.0001307 -> 0.0001287
saving the model at the end of epoch 135, iters 69120
End of epoch 135 / 200 	 Time Taken: 4 sec
learning rate 0.0001287 -> 0.0001267
End of epoch 136 / 200 	 Time Taken: 2 sec
learning rate 0.0001267 -> 0.0001248
End of epoch 137 / 200 	 Time Taken: 2 sec
learning rate 0.0001248 -> 0.0001228
End of epoch 138 / 200 	 Time Taken: 2 sec
learning rate 0.0001228 -> 0.0001208
End of epoch 139 / 200 	 Time Taken: 2 sec
learning rate 0.0001208 -> 0.0001188
saving the model at the end of epoch 140, iters 71680
End of epoch 140 / 200 	 Time Taken: 4 sec
learning rate 0.0001188 -> 0.0001168
End of epoch 141 / 200 	 Time Taken: 2 sec
learning rate 0.0001168 -> 0.0001149
End of epoch 142 / 200 	 Time Taken: 2 sec
learning rate 0.0001149 -> 0.0001129
End of epoch 143 / 200 	 Time Taken: 2 sec
learning rate 0.0001129 -> 0.0001109
End of epoch 144 / 200 	 Time Taken: 2 sec
learning rate 0.0001109 -> 0.0001089
saving the model at the end of epoch 145, iters 74240
End of epoch 145 / 200 	 Time Taken: 4 sec
learning rate 0.0001089 -> 0.0001069
End of epoch 146 / 200 	 Time Taken: 2 sec
learning rate 0.0001069 -> 0.0001050
End of epoch 147 / 200 	 Time Taken: 2 sec
learning rate 0.0001050 -> 0.0001030
End of epoch 148 / 200 	 Time Taken: 2 sec
learning rate 0.0001030 -> 0.0001010
End of epoch 149 / 200 	 Time Taken: 2 sec
learning rate 0.0001010 -> 0.0000990
(epoch: 150, iters: 512, time: 0.002, data: 1.912) G_GAN: 1.190 G_L1: 11.634 D_real: 0.412 D_fake: 0.525 
saving the model at the end of epoch 150, iters 76800
End of epoch 150 / 200 	 Time Taken: 4 sec
learning rate 0.0000990 -> 0.0000970
End of epoch 151 / 200 	 Time Taken: 2 sec
learning rate 0.0000970 -> 0.0000950
End of epoch 152 / 200 	 Time Taken: 2 sec
learning rate 0.0000950 -> 0.0000931
End of epoch 153 / 200 	 Time Taken: 2 sec
learning rate 0.0000931 -> 0.0000911
End of epoch 154 / 200 	 Time Taken: 2 sec
learning rate 0.0000911 -> 0.0000891
saving the model at the end of epoch 155, iters 79360
Traceback (most recent call last):
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 487, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 75, in <module>
    model.save_networks(epoch)
  File "/home/user/swk/pytorch-CycleGAN-and-pix2pix/models/base_model.py", line 157, in save_networks
    torch.save(net.module.cpu().state_dict(), save_path)
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 373, in save
    return
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:274] . unexpected pos 184163520 vs 184163408
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 184163520 vs 184163408
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7efbf3a2f6a7 in /home/user/swk/anacondaFail to get yarn configuration. node:events:353
      throw er; // Unhandled 'error' event
      ^

Error: EBADF: bad file descriptor, read
Emitted 'error' event on ReadStream instance at:
    at emitErrorNT (node:internal/streams/destroy:188:8)
    at errorOrDestroy (node:internal/streams/destroy:251:7)
    at node:internal/fs/streams:193:9
    at FSReqCallback.wrapper [as oncomplete] (node:fs:551:5) {
  errno: -9,
  code: 'EBADF',
  syscall: 'read'
}

[I 2021-05-18 16:08:40.049 ServerApp] jupyterlab | extension was successfully linked.
[W 2021-05-18 16:08:40.053 NotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-05-18 16:08:40.053 NotebookApp] 'certfile' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-05-18 16:08:40.053 NotebookApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-05-18 16:08:40.053 NotebookApp] 'port_retries' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-05-18 16:08:40.053 NotebookApp] 'allow_remote_access' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-05-18 16:08:40.053 NotebookApp] 'allow_origin' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2021-05-18 16:08:40.053 NotebookApp] 'allow_origin' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2021-05-18 16:08:40.186 ServerApp] nbclassic | extension was successfully linked.
[I 2021-05-18 16:08:40.238 ServerApp] The port 8888 is already in use, trying another port.
[I 2021-05-18 16:08:40.242 LabApp] JupyterLab extension loaded from /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/jupyterlab
[I 2021-05-18 16:08:40.242 LabApp] JupyterLab application directory is /home/user/swk/anaconda3/envs/torch38/share/jupyter/lab
[I 2021-05-18 16:08:40.245 ServerApp] jupyterlab | extension was successfully loaded.
[I 2021-05-18 16:08:40.248 ServerApp] nbclassic | extension was successfully loaded.
[I 2021-05-18 16:08:40.248 ServerApp] Serving notebooks from local directory: /home/user/swk/pytorch-CycleGAN-and-pix2pix
[I 2021-05-18 16:08:40.248 ServerApp] Jupyter Server 1.3.0 is running at:
[I 2021-05-18 16:08:40.248 ServerApp] https://175.209.155.38:8889/lab?token=d7ef9df437f0b64e3936d40122013f4845c521eab75c5369
[I 2021-05-18 16:08:40.248 ServerApp]  or https://127.0.0.1:8889/lab?token=d7ef9df437f0b64e3936d40122013f4845c521eab75c5369
[I 2021-05-18 16:08:40.248 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 2021-05-18 16:08:40.252 ServerApp] No web browser found: could not locate runnable browser.
[C 2021-05-18 16:08:40.252 ServerApp] 
    
    To access the server, open this file in a browser:
        file:///home/user/.local/share/jupyter/runtime/jpserver-16777-open.html
    Or copy and paste one of these URLs:
        https://175.209.155.38:8889/lab?token=d7ef9df437f0b64e3936d40122013f4845c521eab75c5369
     or https://127.0.0.1:8889/lab?token=d7ef9df437f0b64e3936d40122013f4845c521eab75c5369
[W 2021-05-18 16:08:40.355 ServerApp] SSL Error on 9 ('112.216.230.34', 54415): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:08:42.435 ServerApp] SSL Error on 10 ('112.216.230.34', 54417): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:08:52.514 ServerApp] SSL Error on 10 ('112.216.230.34', 54419): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:00.002 ServerApp] SSL Error on 10 ('112.216.230.34', 54423): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:00.397 ServerApp] SSL Error on 11 ('112.216.230.34', 54428): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:00.403 ServerApp] SSL Error on 12 ('112.216.230.34', 54430): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:00.404 ServerApp] SSL Error on 13 ('112.216.230.34', 54429): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:00.405 ServerApp] SSL Error on 15 ('112.216.230.34', 54427): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:00.405 ServerApp] SSL Error on 14 ('112.216.230.34', 54431): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[E 2021-05-18 16:09:05.932 LabApp] Fail to get yarn configuration. node:events:353
          throw er; // Unhandled 'error' event
          ^
    
    Error: EBADF: bad file descriptor, read
    Emitted 'error' event on ReadStream instance at:
        at emitErrorNT (node:internal/streams/destroy:188:8)
        at errorOrDestroy (node:internal/streams/destroy:251:7)
        at node:internal/fs/streams:193:9
        at FSReqCallback.wrapper [as oncomplete] (node:fs:551:5) {
      errno: -9,
      code: 'EBADF',
      syscall: 'read'
    }
    
[I 2021-05-18 16:09:06.670 LabApp] Build is up to date
[E 2021-05-18 16:09:07.004 LabApp] Fail to get yarn configuration. node:events:353
          throw er; // Unhandled 'error' event
          ^
    
    Error: EBADF: bad file descriptor, read
    Emitted 'error' event on ReadStream instance at:
        at emitErrorNT (node:internal/streams/destroy:188:8)
        at errorOrDestroy (node:internal/streams/destroy:251:7)
        at node:internal/fs/streams:193:9
        at FSReqCallback.wrapper [as oncomplete] (node:fs:551:5) {
      errno: -9,
      code: 'EBADF',
      syscall: 'read'
    }
    
[I 2021-05-18 16:09:13.763 ServerApp] Kernel started: 8364d4be-5870-4a83-8a58-58f4225f5d2a
[W 2021-05-18 16:09:14.575 ServerApp] SSL Error on 26 ('112.216.230.34', 54474): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:14.630 ServerApp] SSL Error on 26 ('112.216.230.34', 54475): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:09:14.743 ServerApp] SSL Error on 34 ('112.216.230.34', 54477): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
Setting up a new session...
----------------- Options ---------------
               batch_size: 2048                          	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/gaugan/            	[default: None]
             dataset_mode: aligned                       
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: vanilla                       
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 100.0                         
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: pix2pix                       	[default: cycle_gan]
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: gaugan_atob_pix2pix           	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: False                         
                  no_html: False                         
                     norm: batch                         
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 0                             
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 148
initialize network with normal
initialize network with normal
model [Pix2PixModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.414 M
[Network D] Total number of parameters : 2.769 M
-----------------------------------------------
create web directory ./checkpoints/gaugan_atob_pix2pix/web...
learning rate 0.0002000 -> 0.0002000
/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
End of epoch 1 / 200 	 Time Taken: 29 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 2 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 3 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 4 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 5, iters 10240
End of epoch 5 / 200 	 Time Taken: 3 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 6 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 7 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 8 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 9 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 10, iters 20480
End of epoch 10 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 11 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 12 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 13 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 14 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 15, iters 30720
End of epoch 15 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 16 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 17 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 18 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 19 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 20, iters 40960
End of epoch 20 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 21 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 22 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 23 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 24 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 25, iters: 2048, time: 0.000, data: 1.591) G_GAN: 0.898 G_L1: 10.150 D_real: 0.620 D_fake: 0.596 
saving the model at the end of epoch 25, iters 51200
End of epoch 25 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 26 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 27 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 28 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 29 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 30, iters 61440
End of epoch 30 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 31 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 32 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 33 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 34 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 35, iters 71680
End of epoch 35 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 36 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 37 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 38 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 39 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 40, iters 81920
End of epoch 40 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 41 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 42 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 43 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 44 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 45, iters 92160
End of epoch 45 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 46 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 47 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 48 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 49 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 50, iters: 2048, time: 0.000, data: 1.713) G_GAN: 0.926 G_L1: 8.409 D_real: 0.718 D_fake: 0.682 
saving the model at the end of epoch 50, iters 102400
End of epoch 50 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 51 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 52 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 53 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 54 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 55, iters 112640
End of epoch 55 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 56 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 57 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 58 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 59 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 60, iters 122880
Traceback (most recent call last):
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 487, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 75, in <module>
    model.save_networks(epoch)
  File "/home/user/swk/pytorch-CycleGAN-and-pix2pix/models/base_model.py", line 157, in save_networks
    torch.save(net.module.cpu().state_dict(), save_path)
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 373, in save
    return
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:274] . unexpected pos 97612096 vs 97611992
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 97612096 vs 97611992
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7fcda21856a7 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x7fce2e40b500 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x7fce2e4076d3 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x7fce2e40c609 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x7fce2e40d141 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x7fce2e40d935 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x6543f3 (0x7fce3dade3f3 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x2c2c60 (0x7fce3d74cc60 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x2c3dce (0x7fce3d74ddce in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #19: __libc_start_main + 0xe7 (0x7fce4024dbf7 in /lib/x86_64-linux-gnu/libc.so.6)

Setting up a new session...
----------------- Options ---------------
               batch_size: 2048                          	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/gaugan/            	[default: None]
             dataset_mode: aligned                       
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: vanilla                       
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 100.0                         
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: pix2pix                       	[default: cycle_gan]
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: gaugan_atob_pix2pix           	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: False                         
                  no_html: False                         
                     norm: batch                         
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 0                             
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 148
initialize network with normal
initialize network with normal
model [Pix2PixModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.414 M
[Network D] Total number of parameters : 2.769 M
-----------------------------------------------
create web directory ./checkpoints/gaugan_atob_pix2pix/web...
learning rate 0.0002000 -> 0.0002000
/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
End of epoch 1 / 200 	 Time Taken: 30 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 2 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 3 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 4 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 5, iters 10240
End of epoch 5 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 6 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 7 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 8 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 9 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 10, iters 20480
End of epoch 10 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 11 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 12 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 13 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 14 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 15, iters 30720
End of epoch 15 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 16 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 17 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 18 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 19 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 20, iters 40960
End of epoch 20 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 21 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 22 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 23 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 24 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 25, iters: 2048, time: 0.000, data: 1.571) G_GAN: 1.037 G_L1: 11.366 D_real: 0.536 D_fake: 0.552 
saving the model at the end of epoch 25, iters 51200
End of epoch 25 / 200 	 Time Taken: 5 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 26 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 27 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 28 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 29 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 30, iters 61440
End of epoch 30 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 31 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 32 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 33 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 34 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 35, iters 71680
End of epoch 35 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 36 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 37 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 38 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 39 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 40, iters 81920
End of epoch 40 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 41 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 42 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 43 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 44 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 45, iters 92160
End of epoch 45 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 46 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 47 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 48 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 49 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 50, iters: 2048, time: 0.000, data: 1.832) G_GAN: 0.864 G_L1: 8.857 D_real: 0.537 D_fake: 0.782 
saving the model at the end of epoch 50, iters 102400
End of epoch 50 / 200 	 Time Taken: 5 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 51 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 52 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 53 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 54 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 55, iters 112640
End of epoch 55 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 56 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 57 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 58 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 59 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 60, iters 122880
End of epoch 60 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 61 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 62 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 63 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 64 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 65, iters 133120
End of epoch 65 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 66 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 67 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 68 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 69 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 70, iters 143360
End of epoch 70 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 71 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 72 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 73 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 74 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 75, iters: 2048, time: 0.000, data: 1.811) G_GAN: 0.871 G_L1: 6.787 D_real: 0.627 D_fake: 0.625 
saving the model at the end of epoch 75, iters 153600
End of epoch 75 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 76 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 77 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 78 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 79 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 80, iters 163840
End of epoch 80 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 81 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 82 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 83 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 84 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 85, iters 174080
End of epoch 85 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 86 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 87 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 88 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 89 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 90, iters 184320
End of epoch 90 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 91 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 92 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 93 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 94 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 95, iters 194560
End of epoch 95 / 200 	 Time Taken: 4 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 96 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 97 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 98 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 99 / 200 	 Time Taken: 2 sec
learning rate 0.0002000 -> 0.0001980
(epoch: 100, iters: 2048, time: 0.000, data: 1.868) G_GAN: 0.946 G_L1: 6.040 D_real: 0.606 D_fake: 0.564 
saving the model at the end of epoch 100, iters 204800
End of epoch 100 / 200 	 Time Taken: 4 sec
learning rate 0.0001980 -> 0.0001960
End of epoch 101 / 200 	 Time Taken: 2 sec
learning rate 0.0001960 -> 0.0001941
End of epoch 102 / 200 	 Time Taken: 2 sec
learning rate 0.0001941 -> 0.0001921
End of epoch 103 / 200 	 Time Taken: 2 sec
learning rate 0.0001921 -> 0.0001901
End of epoch 104 / 200 	 Time Taken: 2 sec
learning rate 0.0001901 -> 0.0001881
saving the model at the end of epoch 105, iters 215040
End of epoch 105 / 200 	 Time Taken: 4 sec
learning rate 0.0001881 -> 0.0001861
End of epoch 106 / 200 	 Time Taken: 2 sec
learning rate 0.0001861 -> 0.0001842
End of epoch 107 / 200 	 Time Taken: 2 sec
learning rate 0.0001842 -> 0.0001822
End of epoch 108 / 200 	 Time Taken: 2 sec
learning rate 0.0001822 -> 0.0001802
End of epoch 109 / 200 	 Time Taken: 2 sec
learning rate 0.0001802 -> 0.0001782
saving the model at the end of epoch 110, iters 225280
End of epoch 110 / 200 	 Time Taken: 4 sec
learning rate 0.0001782 -> 0.0001762
End of epoch 111 / 200 	 Time Taken: 2 sec
learning rate 0.0001762 -> 0.0001743
End of epoch 112 / 200 	 Time Taken: 2 sec
learning rate 0.0001743 -> 0.0001723
End of epoch 113 / 200 	 Time Taken: 2 sec
learning rate 0.0001723 -> 0.0001703
End of epoch 114 / 200 	 Time Taken: 2 sec
learning rate 0.0001703 -> 0.0001683
saving the model at the end of epoch 115, iters 235520
End of epoch 115 / 200 	 Time Taken: 4 sec
learning rate 0.0001683 -> 0.0001663
End of epoch 116 / 200 	 Time Taken: 2 sec
learning rate 0.0001663 -> 0.0001644
End of epoch 117 / 200 	 Time Taken: 2 sec
learning rate 0.0001644 -> 0.0001624
End of epoch 118 / 200 	 Time Taken: 2 sec
learning rate 0.0001624 -> 0.0001604
End of epoch 119 / 200 	 Time Taken: 2 sec
learning rate 0.0001604 -> 0.0001584
saving the model at the end of epoch 120, iters 245760
End of epoch 120 / 200 	 Time Taken: 4 sec
learning rate 0.0001584 -> 0.0001564
End of epoch 121 / 200 	 Time Taken: 2 sec
learning rate 0.0001564 -> 0.0001545
End of epoch 122 / 200 	 Time Taken: 2 sec
learning rate 0.0001545 -> 0.0001525
End of epoch 123 / 200 	 Time Taken: 2 sec
learning rate 0.0001525 -> 0.0001505
End of epoch 124 / 200 	 Time Taken: 2 sec
learning rate 0.0001505 -> 0.0001485
(epoch: 125, iters: 2048, time: 0.000, data: 1.819) G_GAN: 0.990 G_L1: 5.009 D_real: 0.521 D_fake: 0.520 
saving the model at the end of epoch 125, iters 256000
End of epoch 125 / 200 	 Time Taken: 4 sec
learning rate 0.0001485 -> 0.0001465
End of epoch 126 / 200 	 Time Taken: 2 sec
learning rate 0.0001465 -> 0.0001446
End of epoch 127 / 200 	 Time Taken: 2 sec
learning rate 0.0001446 -> 0.0001426
End of epoch 128 / 200 	 Time Taken: 2 sec
learning rate 0.0001426 -> 0.0001406
End of epoch 129 / 200 	 Time Taken: 2 sec
learning rate 0.0001406 -> 0.0001386
saving the model at the end of epoch 130, iters 266240
End of epoch 130 / 200 	 Time Taken: 4 sec
learning rate 0.0001386 -> 0.0001366
End of epoch 131 / 200 	 Time Taken: 2 sec
learning rate 0.0001366 -> 0.0001347
End of epoch 132 / 200 	 Time Taken: 2 sec
learning rate 0.0001347 -> 0.0001327
End of epoch 133 / 200 	 Time Taken: 2 sec
learning rate 0.0001327 -> 0.0001307
End of epoch 134 / 200 	 Time Taken: 2 sec
learning rate 0.0001307 -> 0.0001287
saving the model at the end of epoch 135, iters 276480
End of epoch 135 / 200 	 Time Taken: 4 sec
learning rate 0.0001287 -> 0.0001267
End of epoch 136 / 200 	 Time Taken: 2 sec
learning rate 0.0001267 -> 0.0001248
End of epoch 137 / 200 	 Time Taken: 2 sec
learning rate 0.0001248 -> 0.0001228
End of epoch 138 / 200 	 Time Taken: 2 sec
learning rate 0.0001228 -> 0.0001208
End of epoch 139 / 200 	 Time Taken: 2 sec
learning rate 0.0001208 -> 0.0001188
saving the model at the end of epoch 140, iters 286720
End of epoch 140 / 200 	 Time Taken: 4 sec
learning rate 0.0001188 -> 0.0001168
End of epoch 141 / 200 	 Time Taken: 2 sec
learning rate 0.0001168 -> 0.0001149
End of epoch 142 / 200 	 Time Taken: 2 sec
learning rate 0.0001149 -> 0.0001129
End of epoch 143 / 200 	 Time Taken: 2 sec
learning rate 0.0001129 -> 0.0001109
End of epoch 144 / 200 	 Time Taken: 2 sec
learning rate 0.0001109 -> 0.0001089
saving the model at the end of epoch 145, iters 296960
End of epoch 145 / 200 	 Time Taken: 4 sec
learning rate 0.0001089 -> 0.0001069
End of epoch 146 / 200 	 Time Taken: 2 sec
learning rate 0.0001069 -> 0.0001050
End of epoch 147 / 200 	 Time Taken: 2 sec
learning rate 0.0001050 -> 0.0001030
End of epoch 148 / 200 	 Time Taken: 2 sec
learning rate 0.0001030 -> 0.0001010
End of epoch 149 / 200 	 Time Taken: 2 sec
learning rate 0.0001010 -> 0.0000990
(epoch: 150, iters: 2048, time: 0.000, data: 1.785) G_GAN: 1.073 G_L1: 4.951 D_real: 0.485 D_fake: 0.531 
saving the model at the end of epoch 150, iters 307200
End of epoch 150 / 200 	 Time Taken: 4 sec
learning rate 0.0000990 -> 0.0000970
End of epoch 151 / 200 	 Time Taken: 2 sec
learning rate 0.0000970 -> 0.0000950
End of epoch 152 / 200 	 Time Taken: 2 sec
learning rate 0.0000950 -> 0.0000931
End of epoch 153 / 200 	 Time Taken: 2 sec
learning rate 0.0000931 -> 0.0000911
End of epoch 154 / 200 	 Time Taken: 2 sec
learning rate 0.0000911 -> 0.0000891
saving the model at the end of epoch 155, iters 317440
Traceback (most recent call last):
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 487, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 75, in <module>
    model.save_networks(epoch)
  File "/home/user/swk/pytorch-CycleGAN-and-pix2pix/models/base_model.py", line 157, in save_networks
    torch.save(net.module.cpu().state_dict(), save_path)
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 373, in save
    return
  File "/home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:274] . unexpected pos 60892224 vs 60892112
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 60892224 vs 60892112
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7f703069f6a7 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x7f70bc925500 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x7f70bc9216d3 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x7f70bc926609 in /home/user/swk/anaconda3/envs/torch38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serializ[W 2021-05-18 16:47:30.276 ServerApp] SSL Error on 9 ('112.216.230.34', 55946): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:47:30.281 ServerApp] SSL Error on 11 ('112.216.230.34', 55944): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:47:30.281 ServerApp] SSL Error on 12 ('112.216.230.34', 55947): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 16:47:30.281 ServerApp] SSL Error on 13 ('112.216.230.34', 55945): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[I 2021-05-18 16:48:35.791 ServerApp] Saving file at /pix2pix.ipynb
[I 2021-05-18 16:50:07.225 ServerApp] Saving file at /pix2pix.ipynb
[I 2021-05-18 16:50:07.992 ServerApp] Saving file at /pix2pix.ipynb
[I 2021-05-18 16:51:21.808 ServerApp] Kernel started: e63ad11c-148e-4321-a893-e63d570a456d
[W 2021-05-18 16:51:21.894 ServerApp] SSL Error on 15 ('112.216.230.34', 56047): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[I 2021-05-18 16:51:30.379 ServerApp] Starting buffering for e63ad11c-148e-4321-a893-e63d570a456d:70e7589a-227a-4b26-b3ea-3a2e076554e4
[W 2021-05-18 17:13:45.467 ServerApp] SSL Error on 9 ('112.216.230.34', 56872): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 17:13:45.468 ServerApp] SSL Error on 11 ('112.216.230.34', 56870): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 17:13:45.470 ServerApp] SSL Error on 12 ('112.216.230.34', 56871): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 17:13:45.472 ServerApp] SSL Error on 13 ('112.216.230.34', 56873): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 17:17:11.643 ServerApp] SSL Error on 15 ('112.216.230.34', 56969): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 17:22:18.086 ServerApp] SSL Error on 9 ('112.216.230.34', 57331): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[I 2021-05-18 17:22:18.292 ServerApp] Saving file at /CycleGAN.ipynb
[W 2021-05-18 17:22:18.356 ServerApp] SSL Error on 11 ('112.216.230.34', 57333): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[W 2021-05-18 17:22:18.388 ServerApp] SSL Error on 12 ('112.216.230.34', 57334): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:1108)
[I 2021-05-18 17:22:18.962 ServerApp] Starting buffering for e63ad11c-148e-4321-a893-e63d570a456d:9e6d2db1-a6d9-47c3-af9d-63bdb0814cb4
[I 2021-05-18 17:22:18.965 ServerApp] Starting buffering for 8364d4be-5870-4a83-8a58-58f4225f5d2a:aab6565e-80e7-4538-bad6-9dce63bef2f7
[W 2021-05-18 18:40:03.586 ServerApp] SSL Error on 9 ('185.156.73.23', 50474): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1108)
[W 2021-05-18 18:40:04.038 ServerApp] SSL Error on 9 ('185.156.73.23', 55396): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1108)
[W 2021-05-18 21:21:47.496 ServerApp] SSL Error on 9 ('128.199.20.132', 61953): [SSL: HTTP_REQUEST] http request (_ssl.c:1108)
[W 2021-05-19 04:26:43.623 ServerApp] SSL Error on 9 ('14.50.234.143', 55425): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1108)
[W 2021-05-19 04:31:55.763 ServerApp] SSL Error on 9 ('185.156.72.12', 62904): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1108)
